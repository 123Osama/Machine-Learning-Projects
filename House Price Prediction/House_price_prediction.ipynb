{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Import Libraries**"
      ],
      "metadata": {
        "id": "GMR5hdoaM_Jf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "kNvDlic4FJj_"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from matplotlib import pyplot as plt\n",
        "import matplotlib\n",
        "matplotlib.rcParams[\"figure.figsize\"]=(20,10)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load Dataset"
      ],
      "metadata": {
        "id": "lDmZfQJ-NI7K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OKn9nEbM8I31",
        "outputId": "69efb6db-b482-4aa9-d551-a8a4d8820789"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df=pd.read_csv(\"/content/drive/MyDrive/ML Projects/House Price Prediction/house_price.csv\")\n",
        "df.head()"
      ],
      "metadata": {
        "id": "CgWzktbeFifE",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 475
        },
        "outputId": "4d11a259-8633-4ead-e942-325a6671a807"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                               title beds bath        area  \\\n",
              "0  Eminent Apartment Of 2200 Sq Ft Is Vacant For ...   3    4   2,200 sqft   \n",
              "1  Apartment Ready To Rent In South Khulshi, Near...   3    4   1,400 sqft   \n",
              "2  Smartly priced 1950 SQ FT apartment, that you ...   3    4   1,950 sqft   \n",
              "3  2000 Sq Ft Residential Apartment Is Up For Ren...   3    3   2,000 sqft   \n",
              "4  Strongly Structured This 1650 Sq. ft Apartment...   3    4   1,650 sqft   \n",
              "\n",
              "                               adress       type   purpose  \\\n",
              "0     Block A, Bashundhara R-A, Dhaka  Apartment  For Rent   \n",
              "1  South Khulshi, Khulshi, Chattogram  Apartment  For Rent   \n",
              "2     Block F, Bashundhara R-A, Dhaka  Apartment  For Rent   \n",
              "3             Sector 9, Uttara, Dhaka  Apartment  For Rent   \n",
              "4     Block I, Bashundhara R-A, Dhaka  Apartment  For Rent   \n",
              "\n",
              "                                            flooPlan  \\\n",
              "0  https://images-cdn.bproperty.com/thumbnails/10...   \n",
              "1  https://images-cdn.bproperty.com/thumbnails/44...   \n",
              "2  https://images-cdn.bproperty.com/thumbnails/11...   \n",
              "3  https://images-cdn.bproperty.com/thumbnails/14...   \n",
              "4  https://images-cdn.bproperty.com/thumbnails/10...   \n",
              "\n",
              "                                                 url        lastUpdated  \\\n",
              "0  https://www.bproperty.com/en/property/details-...    August 13, 2022   \n",
              "1  https://www.bproperty.com/en/property/details-...   January 25, 2022   \n",
              "2  https://www.bproperty.com/en/property/details-...  February 22, 2023   \n",
              "3  https://www.bproperty.com/en/property/details-...   October 28, 2021   \n",
              "4  https://www.bproperty.com/en/property/details-...  February 19, 2023   \n",
              "\n",
              "         price  \n",
              "0  50 Thousand  \n",
              "1  30 Thousand  \n",
              "2  30 Thousand  \n",
              "3  35 Thousand  \n",
              "4  25 Thousand  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-c0631a9a-3003-47d8-9bec-057d422070fa\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>title</th>\n",
              "      <th>beds</th>\n",
              "      <th>bath</th>\n",
              "      <th>area</th>\n",
              "      <th>adress</th>\n",
              "      <th>type</th>\n",
              "      <th>purpose</th>\n",
              "      <th>flooPlan</th>\n",
              "      <th>url</th>\n",
              "      <th>lastUpdated</th>\n",
              "      <th>price</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Eminent Apartment Of 2200 Sq Ft Is Vacant For ...</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>2,200 sqft</td>\n",
              "      <td>Block A, Bashundhara R-A, Dhaka</td>\n",
              "      <td>Apartment</td>\n",
              "      <td>For Rent</td>\n",
              "      <td>https://images-cdn.bproperty.com/thumbnails/10...</td>\n",
              "      <td>https://www.bproperty.com/en/property/details-...</td>\n",
              "      <td>August 13, 2022</td>\n",
              "      <td>50 Thousand</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Apartment Ready To Rent In South Khulshi, Near...</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>1,400 sqft</td>\n",
              "      <td>South Khulshi, Khulshi, Chattogram</td>\n",
              "      <td>Apartment</td>\n",
              "      <td>For Rent</td>\n",
              "      <td>https://images-cdn.bproperty.com/thumbnails/44...</td>\n",
              "      <td>https://www.bproperty.com/en/property/details-...</td>\n",
              "      <td>January 25, 2022</td>\n",
              "      <td>30 Thousand</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Smartly priced 1950 SQ FT apartment, that you ...</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>1,950 sqft</td>\n",
              "      <td>Block F, Bashundhara R-A, Dhaka</td>\n",
              "      <td>Apartment</td>\n",
              "      <td>For Rent</td>\n",
              "      <td>https://images-cdn.bproperty.com/thumbnails/11...</td>\n",
              "      <td>https://www.bproperty.com/en/property/details-...</td>\n",
              "      <td>February 22, 2023</td>\n",
              "      <td>30 Thousand</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2000 Sq Ft Residential Apartment Is Up For Ren...</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>2,000 sqft</td>\n",
              "      <td>Sector 9, Uttara, Dhaka</td>\n",
              "      <td>Apartment</td>\n",
              "      <td>For Rent</td>\n",
              "      <td>https://images-cdn.bproperty.com/thumbnails/14...</td>\n",
              "      <td>https://www.bproperty.com/en/property/details-...</td>\n",
              "      <td>October 28, 2021</td>\n",
              "      <td>35 Thousand</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Strongly Structured This 1650 Sq. ft Apartment...</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>1,650 sqft</td>\n",
              "      <td>Block I, Bashundhara R-A, Dhaka</td>\n",
              "      <td>Apartment</td>\n",
              "      <td>For Rent</td>\n",
              "      <td>https://images-cdn.bproperty.com/thumbnails/10...</td>\n",
              "      <td>https://www.bproperty.com/en/property/details-...</td>\n",
              "      <td>February 19, 2023</td>\n",
              "      <td>25 Thousand</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-c0631a9a-3003-47d8-9bec-057d422070fa')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-c0631a9a-3003-47d8-9bec-057d422070fa button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-c0631a9a-3003-47d8-9bec-057d422070fa');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.tail()"
      ],
      "metadata": {
        "id": "-H3eUzwbFIYk",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 441
        },
        "outputId": "3f3897c1-c140-4aec-ddd1-5ee85767f8a9"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                  title beds bath        area  \\\n",
              "7552  Picture Yourself, Residing In This Well Constr...   4    4   3,600 sqft   \n",
              "7553  Flat For Rent In Uttara Sector 13 Near Lubana ...   3    2     900 sqft   \n",
              "7554  1000 SQ FT flat for rent in Uttara Sector 13 n...   2    2   1,000 sqft   \n",
              "7555  Well Planned Apartment For Rent In Gulshan 1 N...   3    4   3,600 sqft   \n",
              "7556  An Apartment Is Ready For Rent At Baridhara DO...   4    4   2,600 sqft   \n",
              "\n",
              "                         adress       type   purpose  \\\n",
              "7552    Sector 3, Uttara, Dhaka     Duplex  For Rent   \n",
              "7553   Sector 13, Uttara, Dhaka  Apartment  For Rent   \n",
              "7554   Sector 13, Uttara, Dhaka  Apartment  For Rent   \n",
              "7555  Gulshan 1, Gulshan, Dhaka  Apartment  For Rent   \n",
              "7556      Baridhara DOHS, Dhaka  Apartment  For Rent   \n",
              "\n",
              "                                               flooPlan  \\\n",
              "7552  https://images-cdn.bproperty.com/thumbnails/15...   \n",
              "7553  https://images-cdn.bproperty.com/thumbnails/15...   \n",
              "7554  https://images-cdn.bproperty.com/thumbnails/12...   \n",
              "7555  https://images-cdn.bproperty.com/thumbnails/30...   \n",
              "7556  https://images-cdn.bproperty.com/thumbnails/15...   \n",
              "\n",
              "                                                    url        lastUpdated  \\\n",
              "7552  https://www.bproperty.com/en/property/details-...   February 7, 2023   \n",
              "7553  https://www.bproperty.com/en/property/details-...   February 9, 2022   \n",
              "7554  https://www.bproperty.com/en/property/details-...  February 14, 2022   \n",
              "7555  https://www.bproperty.com/en/property/details-...   February 7, 2023   \n",
              "7556  https://www.bproperty.com/en/property/details-...  November 22, 2021   \n",
              "\n",
              "            price  \n",
              "7552  80 Thousand  \n",
              "7553  19 Thousand  \n",
              "7554  22 Thousand  \n",
              "7555    1.75 Lakh  \n",
              "7556  90 Thousand  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-c6f16585-4233-41f4-95b6-4bc4e0206086\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>title</th>\n",
              "      <th>beds</th>\n",
              "      <th>bath</th>\n",
              "      <th>area</th>\n",
              "      <th>adress</th>\n",
              "      <th>type</th>\n",
              "      <th>purpose</th>\n",
              "      <th>flooPlan</th>\n",
              "      <th>url</th>\n",
              "      <th>lastUpdated</th>\n",
              "      <th>price</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>7552</th>\n",
              "      <td>Picture Yourself, Residing In This Well Constr...</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>3,600 sqft</td>\n",
              "      <td>Sector 3, Uttara, Dhaka</td>\n",
              "      <td>Duplex</td>\n",
              "      <td>For Rent</td>\n",
              "      <td>https://images-cdn.bproperty.com/thumbnails/15...</td>\n",
              "      <td>https://www.bproperty.com/en/property/details-...</td>\n",
              "      <td>February 7, 2023</td>\n",
              "      <td>80 Thousand</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7553</th>\n",
              "      <td>Flat For Rent In Uttara Sector 13 Near Lubana ...</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>900 sqft</td>\n",
              "      <td>Sector 13, Uttara, Dhaka</td>\n",
              "      <td>Apartment</td>\n",
              "      <td>For Rent</td>\n",
              "      <td>https://images-cdn.bproperty.com/thumbnails/15...</td>\n",
              "      <td>https://www.bproperty.com/en/property/details-...</td>\n",
              "      <td>February 9, 2022</td>\n",
              "      <td>19 Thousand</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7554</th>\n",
              "      <td>1000 SQ FT flat for rent in Uttara Sector 13 n...</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>1,000 sqft</td>\n",
              "      <td>Sector 13, Uttara, Dhaka</td>\n",
              "      <td>Apartment</td>\n",
              "      <td>For Rent</td>\n",
              "      <td>https://images-cdn.bproperty.com/thumbnails/12...</td>\n",
              "      <td>https://www.bproperty.com/en/property/details-...</td>\n",
              "      <td>February 14, 2022</td>\n",
              "      <td>22 Thousand</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7555</th>\n",
              "      <td>Well Planned Apartment For Rent In Gulshan 1 N...</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>3,600 sqft</td>\n",
              "      <td>Gulshan 1, Gulshan, Dhaka</td>\n",
              "      <td>Apartment</td>\n",
              "      <td>For Rent</td>\n",
              "      <td>https://images-cdn.bproperty.com/thumbnails/30...</td>\n",
              "      <td>https://www.bproperty.com/en/property/details-...</td>\n",
              "      <td>February 7, 2023</td>\n",
              "      <td>1.75 Lakh</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7556</th>\n",
              "      <td>An Apartment Is Ready For Rent At Baridhara DO...</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>2,600 sqft</td>\n",
              "      <td>Baridhara DOHS, Dhaka</td>\n",
              "      <td>Apartment</td>\n",
              "      <td>For Rent</td>\n",
              "      <td>https://images-cdn.bproperty.com/thumbnails/15...</td>\n",
              "      <td>https://www.bproperty.com/en/property/details-...</td>\n",
              "      <td>November 22, 2021</td>\n",
              "      <td>90 Thousand</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-c6f16585-4233-41f4-95b6-4bc4e0206086')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-c6f16585-4233-41f4-95b6-4bc4e0206086 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-c6f16585-4233-41f4-95b6-4bc4e0206086');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preprocessing"
      ],
      "metadata": {
        "id": "z3FVswuKNRT8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.shape\n"
      ],
      "metadata": {
        "id": "kiXfvUJBGF6K",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3080b15f-51e0-46cf-9b1e-e3ca4b28ad5b"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(7557, 11)"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "group_obj = df.groupby('type')\n",
        "\n",
        "# applying aggregation function\n",
        "t_df = group_obj.aggregate('count')\n",
        "t_df.head()"
      ],
      "metadata": {
        "id": "BmfclWOzcPKS",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 175
        },
        "outputId": "c6de8c2f-4b98-49a0-f9c2-a0641762a75d"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "           title  beds  bath  area  adress  purpose  flooPlan   url  \\\n",
              "type                                                                  \n",
              "Apartment   7489  7489  7489  7489    7489     7489      7488  7489   \n",
              "Building      21    21    21    21      21       21        21    21   \n",
              "Duplex        47    47    47    47      47       47        47    47   \n",
              "\n",
              "           lastUpdated  price  \n",
              "type                           \n",
              "Apartment         7489   7489  \n",
              "Building            21     21  \n",
              "Duplex              47     47  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-3dea9e00-5806-4047-a9fc-5f84a1fea980\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>title</th>\n",
              "      <th>beds</th>\n",
              "      <th>bath</th>\n",
              "      <th>area</th>\n",
              "      <th>adress</th>\n",
              "      <th>purpose</th>\n",
              "      <th>flooPlan</th>\n",
              "      <th>url</th>\n",
              "      <th>lastUpdated</th>\n",
              "      <th>price</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>type</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>Apartment</th>\n",
              "      <td>7489</td>\n",
              "      <td>7489</td>\n",
              "      <td>7489</td>\n",
              "      <td>7489</td>\n",
              "      <td>7489</td>\n",
              "      <td>7489</td>\n",
              "      <td>7488</td>\n",
              "      <td>7489</td>\n",
              "      <td>7489</td>\n",
              "      <td>7489</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Building</th>\n",
              "      <td>21</td>\n",
              "      <td>21</td>\n",
              "      <td>21</td>\n",
              "      <td>21</td>\n",
              "      <td>21</td>\n",
              "      <td>21</td>\n",
              "      <td>21</td>\n",
              "      <td>21</td>\n",
              "      <td>21</td>\n",
              "      <td>21</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Duplex</th>\n",
              "      <td>47</td>\n",
              "      <td>47</td>\n",
              "      <td>47</td>\n",
              "      <td>47</td>\n",
              "      <td>47</td>\n",
              "      <td>47</td>\n",
              "      <td>47</td>\n",
              "      <td>47</td>\n",
              "      <td>47</td>\n",
              "      <td>47</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-3dea9e00-5806-4047-a9fc-5f84a1fea980')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-3dea9e00-5806-4047-a9fc-5f84a1fea980 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-3dea9e00-5806-4047-a9fc-5f84a1fea980');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.groupby('type')['type'].agg('count')"
      ],
      "metadata": {
        "id": "k-8ZNuyiGLIC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3301c993-6753-4d77-8a0c-d66f943d8489"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "type\n",
              "Apartment    7489\n",
              "Building       21\n",
              "Duplex         47\n",
              "Name: type, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# df = df.drop(df[df['type'].isin(['building', 'duplex'])].index)\n",
        "# df.head()\n",
        "#df['type']=df['type'].drop(['Building','Duplex'],axis='column')\n",
        "\n",
        "df['type'] = df['type'].str.lower()\n",
        "\n",
        "df = df[~df['type'].isin(['building', 'duplex'])]"
      ],
      "metadata": {
        "id": "oGHUEZA1YF9j"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.groupby('type')['type'].agg('count')"
      ],
      "metadata": {
        "id": "ymFRqB7LzScI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "18b54f28-339f-44b9-d9b5-07125bfd1481"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "type\n",
              "apartment    7489\n",
              "Name: type, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "df1= df.drop(['title','adress','type','purpose','flooPlan','url','lastUpdated'],axis='columns')\n",
        "df1.head()"
      ],
      "metadata": {
        "id": "LYSlE2_uG4qX",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "outputId": "bbe40c2e-ac15-4e2f-8cb9-d4af73b78541"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "  beds bath        area        price\n",
              "0   3    4   2,200 sqft  50 Thousand\n",
              "1   3    4   1,400 sqft  30 Thousand\n",
              "2   3    4   1,950 sqft  30 Thousand\n",
              "3   3    3   2,000 sqft  35 Thousand\n",
              "4   3    4   1,650 sqft  25 Thousand"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-40d4b04b-bff4-4b48-83d1-32952a6c4f46\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>beds</th>\n",
              "      <th>bath</th>\n",
              "      <th>area</th>\n",
              "      <th>price</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>2,200 sqft</td>\n",
              "      <td>50 Thousand</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>1,400 sqft</td>\n",
              "      <td>30 Thousand</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>1,950 sqft</td>\n",
              "      <td>30 Thousand</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>2,000 sqft</td>\n",
              "      <td>35 Thousand</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>1,650 sqft</td>\n",
              "      <td>25 Thousand</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-40d4b04b-bff4-4b48-83d1-32952a6c4f46')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-40d4b04b-bff4-4b48-83d1-32952a6c4f46 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-40d4b04b-bff4-4b48-83d1-32952a6c4f46');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df1.isnull().sum()"
      ],
      "metadata": {
        "id": "Goxl-HaEHhWD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "02363135-802a-4c00-cd3d-e21573ef3f09"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "beds     0\n",
              "bath     0\n",
              "area     0\n",
              "price    0\n",
              "dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df1['area'].unique()"
      ],
      "metadata": {
        "id": "tO_Xcv1IIFB1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "24f0640b-bfc8-436d-e5c0-a2f198dd6642"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['2,200 sqft', '1,400 sqft', '1,950 sqft', '2,000 sqft',\n",
              "       '1,650 sqft', '3,400 sqft', '1,600 sqft', '1,250 sqft',\n",
              "       '2,150 sqft', '1,580 sqft', '3,200 sqft', '3,000 sqft',\n",
              "       '1,800 sqft', '1,750 sqft', '1,310 sqft', '1,700 sqft',\n",
              "       '2,750 sqft', '2,500 sqft', '550 sqft', '1,050 sqft', '1,350 sqft',\n",
              "       '3,590 sqft', '400 sqft', '2,400 sqft', '500 sqft', '2,300 sqft',\n",
              "       '1,200 sqft', '800 sqft', '2,146 sqft', '1,315 sqft', '1,500 sqft',\n",
              "       '700 sqft', '600 sqft', '960 sqft', '4,200 sqft', '2,250 sqft',\n",
              "       '1,517 sqft', '6,300 sqft', '1,407 sqft', '1,850 sqft',\n",
              "       '1,150 sqft', '900 sqft', '1,975 sqft', '1,300 sqft', '1,450 sqft',\n",
              "       '950 sqft', '1,100 sqft', '2,600 sqft', '750 sqft', '2,115 sqft',\n",
              "       '3,600 sqft', '3,150 sqft', '2,100 sqft', '1,319 sqft',\n",
              "       '1,676 sqft', '1,825 sqft', '2,977 sqft', '350 sqft', '1,550 sqft',\n",
              "       '450 sqft', '1,190 sqft', '2,230 sqft', '2,280 sqft', '2,197 sqft',\n",
              "       '1,393 sqft', '1,260 sqft', '650 sqft', '1,075 sqft', '1,365 sqft',\n",
              "       '1,165 sqft', '1,240 sqft', '1,630 sqft', '1,806 sqft',\n",
              "       '1,360 sqft', '2,170 sqft', '1,370 sqft', '2,560 sqft',\n",
              "       '1,615 sqft', '3,350 sqft', '1,375 sqft', '3,500 sqft',\n",
              "       '1,807 sqft', '1,288 sqft', '1,472 sqft', '2,595 sqft',\n",
              "       '1,000 sqft', '2,153 sqft', '1,910 sqft', '2,960 sqft', '720 sqft',\n",
              "       '1,280 sqft', '2,350 sqft', '1,880 sqft', '3,800 sqft',\n",
              "       '1,935 sqft', '3,187 sqft', '1,520 sqft', '4,000 sqft', '920 sqft',\n",
              "       '1,911 sqft', '1,532 sqft', '1,395 sqft', '2,450 sqft',\n",
              "       '1,330 sqft', '2,145 sqft', '4,500 sqft', '1,560 sqft',\n",
              "       '1,599 sqft', '1,793 sqft', '3,300 sqft', '2,550 sqft',\n",
              "       '1,665 sqft', '1,960 sqft', '1,495 sqft', '2,210 sqft',\n",
              "       '2,700 sqft', '2,800 sqft', '2,780 sqft', '2,080 sqft', '850 sqft',\n",
              "       '1,780 sqft', '725 sqft', '1,570 sqft', '420 sqft', '1,980 sqft',\n",
              "       '3,120 sqft', '910 sqft', '300 sqft', '994 sqft', '2,373 sqft',\n",
              "       '1,265 sqft', '1,865 sqft', '2,225 sqft', '1,685 sqft',\n",
              "       '1,171 sqft', '5,400 sqft', '1,722 sqft', '1,420 sqft',\n",
              "       '1,680 sqft', '2,642 sqft', '1,876 sqft', '1,340 sqft',\n",
              "       '1,233 sqft', '1,160 sqft', '3,100 sqft', '2,630 sqft', '730 sqft',\n",
              "       '1,660 sqft', '1,452 sqft', '1,275 sqft', '1,654 sqft',\n",
              "       '1,460 sqft', '4,400 sqft', '1,041 sqft', '2,260 sqft',\n",
              "       '1,230 sqft', '1,114 sqft', '2,092 sqft', '2,015 sqft',\n",
              "       '2,022 sqft', '2,318 sqft', '1,175 sqft', '1,864 sqft',\n",
              "       '1,035 sqft', '1,620 sqft', '1,921 sqft', '2,650 sqft',\n",
              "       '3,680 sqft', '1,860 sqft', '2,435 sqft', '5,000 sqft',\n",
              "       '1,180 sqft', '1,270 sqft', '3,850 sqft', '1,430 sqft',\n",
              "       '1,254 sqft', '1,510 sqft', '2,900 sqft', '1,705 sqft',\n",
              "       '1,426 sqft', '2,465 sqft', '1,425 sqft', '1,485 sqft',\n",
              "       '2,530 sqft', '2,165 sqft', '4,700 sqft', '1,129 sqft', '820 sqft',\n",
              "       '1,558 sqft', '1,060 sqft', '930 sqft', '3,302 sqft', '1,605 sqft',\n",
              "       '2,142 sqft', '3,305 sqft', '1,734 sqft', '2,950 sqft',\n",
              "       '1,900 sqft', '1,020 sqft', '1,512 sqft', '1,785 sqft',\n",
              "       '1,338 sqft', '1,905 sqft', '1,205 sqft', '1,470 sqft',\n",
              "       '1,442 sqft', '2,760 sqft', '1,873 sqft', '2,850 sqft',\n",
              "       '1,285 sqft', '3,765 sqft', '3,700 sqft', '892 sqft', '1,622 sqft',\n",
              "       '1,540 sqft', '1,080 sqft', '1,985 sqft', '1,465 sqft',\n",
              "       '2,088 sqft', '1,312 sqft', '770 sqft', '1,130 sqft', '1,429 sqft',\n",
              "       '2,510 sqft', '1,112 sqft', '1,244 sqft', '1,380 sqft',\n",
              "       '2,040 sqft', '1,325 sqft', '4,100 sqft', '7,000 sqft', '760 sqft',\n",
              "       '2,296 sqft', '1,710 sqft', '1,085 sqft', '1,045 sqft',\n",
              "       '1,070 sqft', '680 sqft', '1,513 sqft', '2,575 sqft', '780 sqft',\n",
              "       '742 sqft', '1,555 sqft', '1,185 sqft', '2,619 sqft', '1,480 sqft',\n",
              "       '768 sqft', '1,440 sqft', '1,140 sqft', '1,566 sqft', '2,090 sqft',\n",
              "       '1,255 sqft', '735 sqft', '1,170 sqft', '1,155 sqft', '1,575 sqft',\n",
              "       '1,302 sqft', '1,225 sqft', '1,328 sqft', '1,345 sqft',\n",
              "       '1,320 sqft', '1,210 sqft', '1,025 sqft', '1,120 sqft', '740 sqft',\n",
              "       '1,383 sqft', '744 sqft', '1,145 sqft', '1,113 sqft', '1,133 sqft',\n",
              "       '1,231 sqft', '1,577 sqft', '2,270 sqft', '1,012 sqft',\n",
              "       '1,156 sqft', '1,386 sqft', '1,525 sqft', '1,271 sqft',\n",
              "       '2,508 sqft', '1,040 sqft', '1,530 sqft', '1,110 sqft',\n",
              "       '1,610 sqft', '2,585 sqft', '845 sqft', '1,505 sqft', '1,154 sqft',\n",
              "       '970 sqft', '626 sqft', '765 sqft', '855 sqft', '1,810 sqft',\n",
              "       '1,065 sqft', '1,125 sqft', '1,645 sqft', '840 sqft', '775 sqft',\n",
              "       '1,030 sqft', '1,015 sqft', '1,253 sqft', '1,286 sqft',\n",
              "       '1,431 sqft', '1,215 sqft', '2,275 sqft', '1,322 sqft',\n",
              "       '1,297 sqft', '1,117 sqft', '1,221 sqft', '1,220 sqft',\n",
              "       '1,952 sqft', '1,172 sqft', '1,625 sqft', '1,435 sqft',\n",
              "       '1,775 sqft', '3,675 sqft', '965 sqft', '1,055 sqft', '520 sqft',\n",
              "       '1,758 sqft', '1,943 sqft', '5,200 sqft', '1,311 sqft',\n",
              "       '1,720 sqft', '4,800 sqft', '1,335 sqft', '2,470 sqft',\n",
              "       '1,770 sqft', '3,554 sqft', '1,104 sqft', '1,827 sqft',\n",
              "       '1,323 sqft', '1,235 sqft', '1,820 sqft', '1,553 sqft',\n",
              "       '1,640 sqft', '2,160 sqft', '752 sqft', '1,930 sqft', '620 sqft',\n",
              "       '1,346 sqft', '1,475 sqft', '1,706 sqft', '1,875 sqft',\n",
              "       '2,148 sqft', '1,331 sqft', '2,190 sqft', '4,099 sqft',\n",
              "       '1,754 sqft', '1,153 sqft', '880 sqft', '1,963 sqft', '3,720 sqft',\n",
              "       '2,159 sqft', '1,010 sqft', '1,124 sqft', '1,675 sqft',\n",
              "       '1,390 sqft', '1,224 sqft', '1,925 sqft', '990 sqft', '1,195 sqft',\n",
              "       '1,187 sqft', '1,166 sqft', '1,304 sqft', '3,721 sqft',\n",
              "       '1,732 sqft', '873 sqft', '823 sqft', '1,523 sqft', '1,772 sqft',\n",
              "       '940 sqft', '5,500 sqft', '1,216 sqft', '2,345 sqft', '1,059 sqft',\n",
              "       '860 sqft', '786 sqft', '824 sqft', '715 sqft', '1,627 sqft',\n",
              "       '2,540 sqft', '1,266 sqft', '2,775 sqft', '1,115 sqft',\n",
              "       '1,515 sqft', '670 sqft', '710 sqft', '2,360 sqft', '755 sqft',\n",
              "       '1,521 sqft', '1,756 sqft', '1,213 sqft', '975 sqft', '1,616 sqft',\n",
              "       '1,578 sqft', '2,667 sqft', '1,695 sqft', '3,750 sqft', '935 sqft',\n",
              "       '3,003 sqft', '1,458 sqft', '1,347 sqft', '1,308 sqft',\n",
              "       '3,050 sqft', '1,686 sqft', '3,900 sqft', '1,535 sqft',\n",
              "       '1,740 sqft', '630 sqft', '560 sqft', '1,556 sqft', '745 sqft',\n",
              "       '785 sqft', '6,600 sqft', '1,352 sqft', '375 sqft', '1,541 sqft',\n",
              "       '1,290 sqft', '2,238 sqft', '1,565 sqft', '1,409 sqft',\n",
              "       '1,677 sqft', '751 sqft', '1,301 sqft', '3,443 sqft', '4,550 sqft',\n",
              "       '1,486 sqft', '2,839 sqft', '3,584 sqft', '1,336 sqft',\n",
              "       '3,515 sqft', '2,004 sqft', '1,762 sqft', '2,730 sqft',\n",
              "       '2,050 sqft', '2,284 sqft', '1,990 sqft', '4,350 sqft',\n",
              "       '1,368 sqft', '3,220 sqft', '2,070 sqft', '885 sqft', '2,325 sqft',\n",
              "       '2,195 sqft', '1,552 sqft', '1,790 sqft', '1,808 sqft',\n",
              "       '2,555 sqft', '830 sqft', '1,436 sqft', '2,365 sqft', '2,485 sqft',\n",
              "       '3,015 sqft', '1,932 sqft', '1,984 sqft', '1,449 sqft',\n",
              "       '1,355 sqft', '1,888 sqft'], dtype=object)"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df1['price'].unique()"
      ],
      "metadata": {
        "id": "LhMQ0h70JGm0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a8bec3b2-9805-4819-93d9-5a6f3761c9c7"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['50 Thousand', '30 Thousand', '35 Thousand', '25 Thousand',\n",
              "       '1.1 Lakh', '23 Thousand', '40 Thousand', '20 Thousand',\n",
              "       '2.5 Lakh', '1.2 Lakh', '21 Thousand', '60 Thousand',\n",
              "       '28 Thousand', '75 Thousand', '55 Thousand', '12 Thousand',\n",
              "       '2 Lakh', '10 Thousand', '18 Thousand', '1.5 Lakh', '85 Thousand',\n",
              "       '70 Thousand', '43 Thousand', '1 Lakh', '80 Thousand',\n",
              "       '45 Thousand', '26 Thousand', '16 Thousand', '14.5 Thousand',\n",
              "       '14 Thousand', '15 Thousand', '90 Thousand', '4 Lakh', '3.5 Lakh',\n",
              "       '27 Thousand', '22 Thousand', '15.5 Thousand', '95 Thousand',\n",
              "       '42 Thousand', '13 Thousand', '11.5 Thousand', '7.3 Thousand',\n",
              "       '1.7 Lakh', '36 Thousand', '32 Thousand', '17 Thousand',\n",
              "       '13.5 Thousand', '34 Thousand', '1.8 Lakh', '65 Thousand',\n",
              "       '1.65 Lakh', '19 Thousand', '18.6 Thousand', '1.3 Lakh',\n",
              "       '47 Thousand', '72 Thousand', '1.6 Lakh', '1.4 Lakh', '9 Thousand',\n",
              "       '1.25 Lakh', '3 Lakh', '37 Thousand', '16.5 Thousand',\n",
              "       '9.5 Thousand', '33 Thousand', '4.5 Lakh', '63 Thousand',\n",
              "       '11 Thousand', '19.05 Thousand', '12.5 Thousand', '38 Thousand',\n",
              "       '17.5 Thousand', '1.35 Lakh', '1.9 Lakh', '24 Thousand',\n",
              "       '35.5 Thousand', '6 Thousand', '18.5 Thousand', '5 Lakh',\n",
              "       '39 Thousand', '8 Thousand', '5.5 Thousand', '71 Thousand',\n",
              "       '31 Thousand', '12.8 Thousand', '52 Thousand', '1.15 Lakh',\n",
              "       '2.2 Lakh', '46 Thousand', '2.9 Lakh', '58 Thousand', '2.15 Lakh',\n",
              "       '88 Thousand', '11.3 Thousand', '57 Thousand', '2.3 Lakh',\n",
              "       '1.85 Lakh', '2.6 Lakh', '2.8 Lakh', '10.5 Thousand',\n",
              "       '29.5 Thousand', '8.5 Thousand', '17.24 Thousand', '3.4 Lakh',\n",
              "       '29 Thousand', '3.6 Lakh', '20.2 Thousand', '20.5 Thousand',\n",
              "       '44 Thousand', '13.2 Thousand', '25.5 Thousand', '26.5 Thousand',\n",
              "       '10.2 Thousand', '11.01 Thousand', '6.5 Thousand', '3.2 Lakh',\n",
              "       '28.5 Thousand', '21.5 Thousand', '10.7 Thousand', '32.5 Thousand',\n",
              "       '18.8 Thousand', '18.3 Thousand', '33.2 Thousand', '8.2 Thousand',\n",
              "       '7 Thousand', '6.2 Thousand', '22.5 Thousand', '7.5 Thousand',\n",
              "       '11.8 Thousand', '11.7 Thousand', '23.5 Thousand', '2.4 Lakh',\n",
              "       '6 Lakh', '19.3 Thousand', '18.4 Thousand', '10.6 Thousand',\n",
              "       '16.3 Thousand', '68 Thousand', '59 Thousand', '9.24 Lakh',\n",
              "       '1.37 Lakh', '78 Thousand', '1.05 Lakh', '1.45 Lakh', '1.75 Lakh'],\n",
              "      dtype=object)"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lakh_count = df1['price'].str.count('Lakh').sum()\n",
        "print(lakh_count)"
      ],
      "metadata": {
        "id": "e6wmjxPRN9-3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a444df9e-f5f6-4e04-f40d-dddc76403c2b"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "378\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def unit_convert(x):\n",
        "    if 'Lakh' in x:\n",
        "        return float(x.split()[0]) * 1000\n",
        "    else:\n",
        "        return x"
      ],
      "metadata": {
        "id": "3YpYi8dXC1yt"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df1['price'] = df1['price'].apply(unit_convert)"
      ],
      "metadata": {
        "id": "1qdSdniPETAa"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "K0msUyLjEPlU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df1['price'].unique()"
      ],
      "metadata": {
        "id": "KHAtUQUcM4hy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fbb2c9e3-be93-4bf3-ef2a-343eb57b1754"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['50 Thousand', '30 Thousand', '35 Thousand', '25 Thousand', 1100.0,\n",
              "       '23 Thousand', '40 Thousand', '20 Thousand', 2500.0, 1200.0,\n",
              "       '21 Thousand', '60 Thousand', '28 Thousand', '75 Thousand',\n",
              "       '55 Thousand', '12 Thousand', 2000.0, '10 Thousand', '18 Thousand',\n",
              "       1500.0, '85 Thousand', '70 Thousand', '43 Thousand', 1000.0,\n",
              "       '80 Thousand', '45 Thousand', '26 Thousand', '16 Thousand',\n",
              "       '14.5 Thousand', '14 Thousand', '15 Thousand', '90 Thousand',\n",
              "       4000.0, 3500.0, '27 Thousand', '22 Thousand', '15.5 Thousand',\n",
              "       '95 Thousand', '42 Thousand', '13 Thousand', '11.5 Thousand',\n",
              "       '7.3 Thousand', 1700.0, '36 Thousand', '32 Thousand',\n",
              "       '17 Thousand', '13.5 Thousand', '34 Thousand', 1800.0,\n",
              "       '65 Thousand', 1650.0, '19 Thousand', '18.6 Thousand', 1300.0,\n",
              "       '47 Thousand', '72 Thousand', 1600.0, 1400.0, '9 Thousand', 1250.0,\n",
              "       3000.0, '37 Thousand', '16.5 Thousand', '9.5 Thousand',\n",
              "       '33 Thousand', 4500.0, '63 Thousand', '11 Thousand',\n",
              "       '19.05 Thousand', '12.5 Thousand', '38 Thousand', '17.5 Thousand',\n",
              "       1350.0, 1900.0, '24 Thousand', '35.5 Thousand', '6 Thousand',\n",
              "       '18.5 Thousand', 5000.0, '39 Thousand', '8 Thousand',\n",
              "       '5.5 Thousand', '71 Thousand', '31 Thousand', '12.8 Thousand',\n",
              "       '52 Thousand', 1150.0, 2200.0, '46 Thousand', 2900.0,\n",
              "       '58 Thousand', 2150.0, '88 Thousand', '11.3 Thousand',\n",
              "       '57 Thousand', 2300.0, 1850.0, 2600.0, 2800.0, '10.5 Thousand',\n",
              "       '29.5 Thousand', '8.5 Thousand', '17.24 Thousand', 3400.0,\n",
              "       '29 Thousand', 3600.0, '20.2 Thousand', '20.5 Thousand',\n",
              "       '44 Thousand', '13.2 Thousand', '25.5 Thousand', '26.5 Thousand',\n",
              "       '10.2 Thousand', '11.01 Thousand', '6.5 Thousand', 3200.0,\n",
              "       '28.5 Thousand', '21.5 Thousand', '10.7 Thousand', '32.5 Thousand',\n",
              "       '18.8 Thousand', '18.3 Thousand', '33.2 Thousand', '8.2 Thousand',\n",
              "       '7 Thousand', '6.2 Thousand', '22.5 Thousand', '7.5 Thousand',\n",
              "       '11.8 Thousand', '11.7 Thousand', '23.5 Thousand', 2400.0, 6000.0,\n",
              "       '19.3 Thousand', '18.4 Thousand', '10.6 Thousand', '16.3 Thousand',\n",
              "       '68 Thousand', '59 Thousand', 9240.0, 1370.0, '78 Thousand',\n",
              "       1050.0, 1450.0, 1750.0], dtype=object)"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lakh_count = df1['price'].str.count('Lakh').sum()\n",
        "print(lakh_count)"
      ],
      "metadata": {
        "id": "gz2D5B4HSgTU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dce73ca0-8f4c-44f1-8bf4-e7ec42695c38"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df1.head(20)"
      ],
      "metadata": {
        "id": "jFXBzi2OU5V8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 676
        },
        "outputId": "c55822fb-ef68-4cd7-f4b1-c571a0efaf67"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   beds bath        area        price\n",
              "0    3    4   2,200 sqft  50 Thousand\n",
              "1    3    4   1,400 sqft  30 Thousand\n",
              "2    3    4   1,950 sqft  30 Thousand\n",
              "3    3    3   2,000 sqft  35 Thousand\n",
              "4    3    4   1,650 sqft  25 Thousand\n",
              "5    5    5   3,400 sqft       1100.0\n",
              "6    3    3   1,600 sqft  35 Thousand\n",
              "7    3    3   1,250 sqft  23 Thousand\n",
              "8    3    4   2,150 sqft  40 Thousand\n",
              "9    3    3   1,250 sqft  23 Thousand\n",
              "10   3    3   1,250 sqft  23 Thousand\n",
              "11   3    3   1,580 sqft  20 Thousand\n",
              "12   4    4   3,200 sqft       2500.0\n",
              "13   3    3   3,000 sqft       1200.0\n",
              "14   3    3   1,250 sqft  21 Thousand\n",
              "15   3    4   1,800 sqft  60 Thousand\n",
              "16   3    3   1,250 sqft  21 Thousand\n",
              "17   3    3   2,000 sqft  40 Thousand\n",
              "18   3    4   1,750 sqft  30 Thousand\n",
              "19   3    3   1,250 sqft  23 Thousand"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-eab506d3-d329-43f9-8d1c-fb7ecd2aa9ea\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>beds</th>\n",
              "      <th>bath</th>\n",
              "      <th>area</th>\n",
              "      <th>price</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>2,200 sqft</td>\n",
              "      <td>50 Thousand</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>1,400 sqft</td>\n",
              "      <td>30 Thousand</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>1,950 sqft</td>\n",
              "      <td>30 Thousand</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>2,000 sqft</td>\n",
              "      <td>35 Thousand</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>1,650 sqft</td>\n",
              "      <td>25 Thousand</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>3,400 sqft</td>\n",
              "      <td>1100.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>1,600 sqft</td>\n",
              "      <td>35 Thousand</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>1,250 sqft</td>\n",
              "      <td>23 Thousand</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>2,150 sqft</td>\n",
              "      <td>40 Thousand</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>1,250 sqft</td>\n",
              "      <td>23 Thousand</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>1,250 sqft</td>\n",
              "      <td>23 Thousand</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>1,580 sqft</td>\n",
              "      <td>20 Thousand</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>3,200 sqft</td>\n",
              "      <td>2500.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3,000 sqft</td>\n",
              "      <td>1200.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>1,250 sqft</td>\n",
              "      <td>21 Thousand</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>1,800 sqft</td>\n",
              "      <td>60 Thousand</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>1,250 sqft</td>\n",
              "      <td>21 Thousand</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>2,000 sqft</td>\n",
              "      <td>40 Thousand</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>1,750 sqft</td>\n",
              "      <td>30 Thousand</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>1,250 sqft</td>\n",
              "      <td>23 Thousand</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-eab506d3-d329-43f9-8d1c-fb7ecd2aa9ea')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-eab506d3-d329-43f9-8d1c-fb7ecd2aa9ea button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-eab506d3-d329-43f9-8d1c-fb7ecd2aa9ea');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "df1['prices'] = df1['price'].apply(lambda x: round(float(str(x).replace(',', '').split(' ')[0])) if isinstance(x, str) else round(x))\n",
        "df1['areas'] = df1['area'].apply(lambda x: round(float(str(x).replace(',', '').split(' ')[0])) if isinstance(x, str) else round(x))\n",
        "\n"
      ],
      "metadata": {
        "id": "JlXqK3mKIZ4_"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df1.head()"
      ],
      "metadata": {
        "id": "2OgeOQ6iJKe4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "outputId": "0cad0919-476e-4ae8-b5ec-086971091d03"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "  beds bath        area        price  prices  areas\n",
              "0   3    4   2,200 sqft  50 Thousand      50   2200\n",
              "1   3    4   1,400 sqft  30 Thousand      30   1400\n",
              "2   3    4   1,950 sqft  30 Thousand      30   1950\n",
              "3   3    3   2,000 sqft  35 Thousand      35   2000\n",
              "4   3    4   1,650 sqft  25 Thousand      25   1650"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-073f2d07-9169-435f-8839-50b025ca5e87\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>beds</th>\n",
              "      <th>bath</th>\n",
              "      <th>area</th>\n",
              "      <th>price</th>\n",
              "      <th>prices</th>\n",
              "      <th>areas</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>2,200 sqft</td>\n",
              "      <td>50 Thousand</td>\n",
              "      <td>50</td>\n",
              "      <td>2200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>1,400 sqft</td>\n",
              "      <td>30 Thousand</td>\n",
              "      <td>30</td>\n",
              "      <td>1400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>1,950 sqft</td>\n",
              "      <td>30 Thousand</td>\n",
              "      <td>30</td>\n",
              "      <td>1950</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>2,000 sqft</td>\n",
              "      <td>35 Thousand</td>\n",
              "      <td>35</td>\n",
              "      <td>2000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>1,650 sqft</td>\n",
              "      <td>25 Thousand</td>\n",
              "      <td>25</td>\n",
              "      <td>1650</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-073f2d07-9169-435f-8839-50b025ca5e87')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-073f2d07-9169-435f-8839-50b025ca5e87 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-073f2d07-9169-435f-8839-50b025ca5e87');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df2= df1.drop(['area','price'],axis='columns')"
      ],
      "metadata": {
        "id": "4fHbfW9CJtGj"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df2.head()"
      ],
      "metadata": {
        "id": "Q8rq1zA0KJL7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "outputId": "1e052217-4abb-40b1-afda-53e237f4063b"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "  beds bath  prices  areas\n",
              "0   3    4       50   2200\n",
              "1   3    4       30   1400\n",
              "2   3    4       30   1950\n",
              "3   3    3       35   2000\n",
              "4   3    4       25   1650"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-1b6fe055-31eb-4772-b57f-449e263fec41\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>beds</th>\n",
              "      <th>bath</th>\n",
              "      <th>prices</th>\n",
              "      <th>areas</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>50</td>\n",
              "      <td>2200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>30</td>\n",
              "      <td>1400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>30</td>\n",
              "      <td>1950</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>35</td>\n",
              "      <td>2000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>25</td>\n",
              "      <td>1650</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-1b6fe055-31eb-4772-b57f-449e263fec41')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-1b6fe055-31eb-4772-b57f-449e263fec41 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-1b6fe055-31eb-4772-b57f-449e263fec41');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df2['prices'].unique()"
      ],
      "metadata": {
        "id": "xwqucMqVKTS_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "44039a62-70e6-48d9-dca8-b4264344b822"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([  50,   30,   35,   25, 1100,   23,   40,   20, 2500, 1200,   21,\n",
              "         60,   28,   75,   55,   12, 2000,   10,   18, 1500,   85,   70,\n",
              "         43, 1000,   80,   45,   26,   16,   14,   15,   90, 4000, 3500,\n",
              "         27,   22,   95,   42,   13,    7, 1700,   36,   32,   17,   34,\n",
              "       1800,   65, 1650,   19, 1300,   47,   72, 1600, 1400,    9, 1250,\n",
              "       3000,   37,   33, 4500,   63,   11,   38, 1350, 1900,   24,    6,\n",
              "       5000,   39,    8,   71,   31,   52, 1150, 2200,   46, 2900,   58,\n",
              "       2150,   88,   57, 2300, 1850, 2600, 2800, 3400,   29, 3600,   44,\n",
              "       3200, 2400, 6000,   68,   59, 9240, 1370,   78, 1050, 1450, 1750])"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df2.shape"
      ],
      "metadata": {
        "id": "mVk8WcdtXEv8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "df20cb2c-9b7e-40b6-af74-71d6176fb603"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(7489, 4)"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df2['beds'].unique()"
      ],
      "metadata": {
        "id": "aiLaGuIDIHSb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c7138c6c-ea4b-44d9-868c-071d0153cb0b"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['3 ', '5 ', '4 ', '1 Bed', '2 ', '6 '], dtype=object)"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "x3Wwln21jslO"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "df2['beds'] = df2['beds'].apply(lambda x: re.sub(r'(?i)\\s*bed\\s*', '', x))\n",
        "df2['bath'] = df2['bath'].apply(lambda x: re.sub(r'(?i)\\s*bath\\s*', '', x))\n",
        "\n"
      ],
      "metadata": {
        "id": "uM_DRQ5vgG9h"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df2['beds'].unique()\n"
      ],
      "metadata": {
        "id": "dTW6v5sNhY6Z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6399f333-8b29-4601-f130-753ffa92f71c"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['3 ', '5 ', '4 ', '1', '2 ', '6 '], dtype=object)"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df2['bath'].unique()"
      ],
      "metadata": {
        "id": "H9vR9dKuhbBU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "482ee1e8-dfb6-4d66-dab0-1370503b5385"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['4 ', '3 ', '5 ', '2 ', '1', '6 '], dtype=object)"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df2.head()"
      ],
      "metadata": {
        "id": "PPthbVgGjhu2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "outputId": "56e7c47d-bfbc-4327-ddd5-11418be76eae"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "  beds bath  prices  areas\n",
              "0   3    4       50   2200\n",
              "1   3    4       30   1400\n",
              "2   3    4       30   1950\n",
              "3   3    3       35   2000\n",
              "4   3    4       25   1650"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-34af466c-a2e3-4bcf-8a1d-da79e31dfcc0\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>beds</th>\n",
              "      <th>bath</th>\n",
              "      <th>prices</th>\n",
              "      <th>areas</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>50</td>\n",
              "      <td>2200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>30</td>\n",
              "      <td>1400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>30</td>\n",
              "      <td>1950</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>35</td>\n",
              "      <td>2000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>25</td>\n",
              "      <td>1650</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-34af466c-a2e3-4bcf-8a1d-da79e31dfcc0')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-34af466c-a2e3-4bcf-8a1d-da79e31dfcc0 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-34af466c-a2e3-4bcf-8a1d-da79e31dfcc0');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nan_columns = df2[['beds', 'bath', 'prices', 'areas']].isna().any()\n",
        "\n",
        "# Print the columns with NaN values\n",
        "print(nan_columns)"
      ],
      "metadata": {
        "id": "pKg7_HbboMja",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "89be34a2-cb57-4ac9-9f76-495f47917520"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "beds      False\n",
            "bath      False\n",
            "prices    False\n",
            "areas     False\n",
            "dtype: bool\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df2 = df2.dropna(subset=['beds', 'bath', 'prices', 'areas'])"
      ],
      "metadata": {
        "id": "Tdwhth6eoeZB"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df2 = df2.iloc[:, [0, 1, 3, 2]]"
      ],
      "metadata": {
        "id": "ug7jmAC9-T8Z"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df3=df2.copy()"
      ],
      "metadata": {
        "id": "grevt4b7mPVu"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df3.head()"
      ],
      "metadata": {
        "id": "NaGoasxwms0p",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "outputId": "50c58d1f-e6a8-4089-c701-e26320952624"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "  beds bath  areas  prices\n",
              "0   3    4    2200      50\n",
              "1   3    4    1400      30\n",
              "2   3    4    1950      30\n",
              "3   3    3    2000      35\n",
              "4   3    4    1650      25"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-d8a43c4b-b2d8-4a8f-b847-6c0e65293428\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>beds</th>\n",
              "      <th>bath</th>\n",
              "      <th>areas</th>\n",
              "      <th>prices</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>2200</td>\n",
              "      <td>50</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>1400</td>\n",
              "      <td>30</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>1950</td>\n",
              "      <td>30</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>2000</td>\n",
              "      <td>35</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>1650</td>\n",
              "      <td>25</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-d8a43c4b-b2d8-4a8f-b847-6c0e65293428')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-d8a43c4b-b2d8-4a8f-b847-6c0e65293428 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-d8a43c4b-b2d8-4a8f-b847-6c0e65293428');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df3.shape"
      ],
      "metadata": {
        "id": "Q9nq5DRUpfTG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e254abed-8810-495f-a62e-71dabbeda5f4"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(7489, 4)"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "x = df3[['beds', 'bath', 'areas']]\n",
        "y = df3[['prices']]\n",
        "\n",
        "scaler = MinMaxScaler()\n",
        "x_scaled = scaler.fit_transform(x)\n",
        "\n",
        "df_scaled = pd.DataFrame(x_scaled, columns=['beds', 'bath', 'area'])\n",
        "df_scaled['price'] = y"
      ],
      "metadata": {
        "id": "AtzgXcn29idh"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_scaled.head()"
      ],
      "metadata": {
        "id": "d9OQMNEY9mN5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "outputId": "c374889e-489f-4d9a-dd64-125df1ff0cd0"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   beds  bath      area  price\n",
              "0   0.4   0.6  0.283582   50.0\n",
              "1   0.4   0.6  0.164179   30.0\n",
              "2   0.4   0.6  0.246269   30.0\n",
              "3   0.4   0.4  0.253731   35.0\n",
              "4   0.4   0.6  0.201493   25.0"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-5c8137fd-c8c8-498b-a672-a5a29f1fa155\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>beds</th>\n",
              "      <th>bath</th>\n",
              "      <th>area</th>\n",
              "      <th>price</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.4</td>\n",
              "      <td>0.6</td>\n",
              "      <td>0.283582</td>\n",
              "      <td>50.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.4</td>\n",
              "      <td>0.6</td>\n",
              "      <td>0.164179</td>\n",
              "      <td>30.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.4</td>\n",
              "      <td>0.6</td>\n",
              "      <td>0.246269</td>\n",
              "      <td>30.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.4</td>\n",
              "      <td>0.4</td>\n",
              "      <td>0.253731</td>\n",
              "      <td>35.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.4</td>\n",
              "      <td>0.6</td>\n",
              "      <td>0.201493</td>\n",
              "      <td>25.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-5c8137fd-c8c8-498b-a672-a5a29f1fa155')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-5c8137fd-c8c8-498b-a672-a5a29f1fa155 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-5c8137fd-c8c8-498b-a672-a5a29f1fa155');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_scaled[['beds', 'bath', 'area','price']].isna().any()"
      ],
      "metadata": {
        "id": "jZIUamOsCYLG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "24714684-b07b-490d-d2e5-7d7b0c3bdf57"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "beds     False\n",
              "bath     False\n",
              "area     False\n",
              "price     True\n",
              "dtype: bool"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_scaled = df_scaled.dropna(subset=['beds', 'bath', 'area','price'])"
      ],
      "metadata": {
        "id": "Fj_EQAcSCiKo"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_scaled[['beds', 'bath', 'area','price']].isna().any()"
      ],
      "metadata": {
        "id": "FHQ5It2WCone",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d85594fd-0f02-42c9-d13e-4ad889622a28"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "beds     False\n",
              "bath     False\n",
              "area     False\n",
              "price    False\n",
              "dtype: bool"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_scaled.head()"
      ],
      "metadata": {
        "id": "KyJMoo6NC3Bm",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "outputId": "cfc4598c-4b3c-40a5-e469-cc961f3226ea"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   beds  bath      area  price\n",
              "0   0.4   0.6  0.283582   50.0\n",
              "1   0.4   0.6  0.164179   30.0\n",
              "2   0.4   0.6  0.246269   30.0\n",
              "3   0.4   0.4  0.253731   35.0\n",
              "4   0.4   0.6  0.201493   25.0"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-3b51ae78-bf51-4702-9daf-1bb7ba717343\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>beds</th>\n",
              "      <th>bath</th>\n",
              "      <th>area</th>\n",
              "      <th>price</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.4</td>\n",
              "      <td>0.6</td>\n",
              "      <td>0.283582</td>\n",
              "      <td>50.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.4</td>\n",
              "      <td>0.6</td>\n",
              "      <td>0.164179</td>\n",
              "      <td>30.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.4</td>\n",
              "      <td>0.6</td>\n",
              "      <td>0.246269</td>\n",
              "      <td>30.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.4</td>\n",
              "      <td>0.4</td>\n",
              "      <td>0.253731</td>\n",
              "      <td>35.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.4</td>\n",
              "      <td>0.6</td>\n",
              "      <td>0.201493</td>\n",
              "      <td>25.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-3b51ae78-bf51-4702-9daf-1bb7ba717343')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-3b51ae78-bf51-4702-9daf-1bb7ba717343 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-3b51ae78-bf51-4702-9daf-1bb7ba717343');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X = df_scaled[['beds', 'bath', 'area']].values\n",
        "y = df_scaled['price'].values"
      ],
      "metadata": {
        "id": "DKA66jr3DIu_"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X.dtype"
      ],
      "metadata": {
        "id": "S_JoF6FkDLTJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d66efd76-8b78-4c30-ac85-b08ea5a33387"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dtype('float64')"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "eI5cXdZNDTpW"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_cost(X, y, w, b):\n",
        "    m = X.shape[0]\n",
        "    cost = 0.0\n",
        "    for i in range(m):\n",
        "        f_wb_i = np.dot(X[i], w) + b\n",
        "        cost = cost + (f_wb_i - y[i])**2\n",
        "    cost = cost / (2 * m)\n",
        "    return cost"
      ],
      "metadata": {
        "id": "Kn1k5KZs_D5N"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "initial_w = 2\n",
        "initial_b = 1\n",
        "\n",
        "cost = compute_cost(X_train, y_train, initial_w,initial_b)\n",
        "print(cost)"
      ],
      "metadata": {
        "id": "iNiWwxVWDeWD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "439f5c36-128b-4ad0-fc03-c4d2fb4dd78f"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[104895.32425661 104896.70945951 104936.04343574]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_gradient(X, y, w, b):\n",
        "    m,n = X.shape           #(number of examples, number of features)\n",
        "    dj_dw = np.zeros((n,))\n",
        "    dj_db = 0.\n",
        "\n",
        "    for i in range(m):\n",
        "        err = (np.dot(X[i], w) + b) - y[i]\n",
        "        for j in range(n):\n",
        "            dj_dw[j] = dj_dw[j] + err * X[i, j]\n",
        "        dj_db = dj_db + err\n",
        "    dj_dw = dj_dw / m\n",
        "    dj_db = dj_db / m\n",
        "\n",
        "    return dj_db, dj_dw"
      ],
      "metadata": {
        "id": "xXLyfNwx_Vdf"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def gradient_descent(X, y, initial_w, initial_b, cost_function, gradient_function, learning_rate, epochs):\n",
        "    costs = []\n",
        "    w = initial_w.copy()\n",
        "    b = initial_b\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        dj_db, dj_dw = gradient_function(X, y, w, b)\n",
        "\n",
        "        w = w - learning_rate * dj_dw\n",
        "        b = b - learning_rate * dj_db\n",
        "\n",
        "        costs.append(cost_function(X, y, w, b))\n",
        "        print(f\"Epoch {epoch+1}/{epochs} - Training Loss: {cost}\")\n",
        "\n",
        "    return w, b, costs"
      ],
      "metadata": {
        "id": "kKSBJ1V5DphZ"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "w_init = np.array([-1, 2, 3])\n",
        "initial_w = np.zeros_like(w_init)\n",
        "initial_b = 2\n",
        "epochs = 1000\n",
        "learning_rate = .001\n",
        "w_final, b_final, cost_final = gradient_descent(X_train, y_train, initial_w, initial_b, compute_cost, compute_gradient, learning_rate, epochs)\n",
        "\n",
        "print(f\"b,w {b_final:0.2f},{w_final} \")"
      ],
      "metadata": {
        "id": "cgZPQhzdAwNP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "884d9dda-041b-4e02-cd94-dd1713ea48c5"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 2/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 3/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 4/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 5/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 6/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 7/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 8/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 9/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 10/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 11/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 12/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 13/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 14/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 15/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 16/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 17/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 18/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 19/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 20/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 21/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 22/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 23/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 24/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 25/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 26/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 27/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 28/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 29/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 30/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 31/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 32/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 33/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 34/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 35/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 36/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 37/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 38/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 39/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 40/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 41/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 42/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 43/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 44/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 45/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 46/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 47/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 48/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 49/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 50/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 51/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 52/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 53/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 54/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 55/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 56/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 57/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 58/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 59/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 60/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 61/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 62/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 63/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 64/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 65/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 66/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 67/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 68/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 69/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 70/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 71/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 72/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 73/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 74/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 75/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 76/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 77/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 78/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 79/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 80/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 81/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 82/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 83/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 84/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 85/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 86/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 87/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 88/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 89/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 90/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 91/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 92/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 93/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 94/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 95/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 96/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 97/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 98/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 99/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 100/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 101/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 102/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 103/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 104/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 105/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 106/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 107/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 108/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 109/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 110/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 111/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 112/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 113/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 114/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 115/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 116/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 117/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 118/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 119/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 120/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 121/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 122/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 123/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 124/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 125/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 126/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 127/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 128/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 129/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 130/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 131/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 132/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 133/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 134/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 135/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 136/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 137/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 138/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 139/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 140/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 141/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 142/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 143/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 144/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 145/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 146/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 147/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 148/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 149/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 150/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 151/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 152/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 153/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 154/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 155/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 156/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 157/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 158/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 159/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 160/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 161/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 162/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 163/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 164/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 165/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 166/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 167/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 168/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 169/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 170/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 171/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 172/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 173/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 174/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 175/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 176/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 177/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 178/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 179/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 180/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 181/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 182/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 183/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 184/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 185/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 186/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 187/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 188/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 189/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 190/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 191/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 192/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 193/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 194/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 195/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 196/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 197/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 198/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 199/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 200/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 201/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 202/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 203/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 204/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 205/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 206/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 207/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 208/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 209/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 210/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 211/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 212/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 213/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 214/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 215/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 216/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 217/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 218/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 219/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 220/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 221/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 222/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 223/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 224/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 225/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 226/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 227/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 228/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 229/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 230/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 231/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 232/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 233/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 234/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 235/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 236/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 237/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 238/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 239/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 240/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 241/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 242/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 243/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 244/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 245/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 246/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 247/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 248/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 249/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 250/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 251/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 252/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 253/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 254/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 255/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 256/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 257/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 258/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 259/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 260/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 261/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 262/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 263/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 264/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 265/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 266/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 267/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 268/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 269/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 270/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 271/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 272/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 273/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 274/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 275/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 276/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 277/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 278/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 279/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 280/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 281/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 282/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 283/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 284/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 285/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 286/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 287/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 288/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 289/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 290/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 291/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 292/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 293/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 294/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 295/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 296/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 297/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 298/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 299/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 300/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 301/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 302/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 303/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 304/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 305/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 306/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 307/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 308/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 309/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 310/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 311/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 312/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 313/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 314/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 315/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 316/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 317/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 318/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 319/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 320/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 321/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 322/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 323/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 324/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 325/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 326/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 327/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 328/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 329/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 330/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 331/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 332/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 333/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 334/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 335/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 336/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 337/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 338/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 339/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 340/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 341/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 342/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 343/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 344/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 345/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 346/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 347/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 348/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 349/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 350/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 351/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 352/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 353/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 354/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 355/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 356/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 357/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 358/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 359/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 360/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 361/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 362/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 363/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 364/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 365/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 366/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 367/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 368/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 369/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 370/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 371/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 372/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 373/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 374/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 375/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 376/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 377/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 378/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 379/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 380/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 381/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 382/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 383/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 384/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 385/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 386/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 387/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 388/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 389/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 390/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 391/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 392/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 393/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 394/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 395/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 396/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 397/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 398/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 399/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 400/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 401/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 402/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 403/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 404/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 405/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 406/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 407/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 408/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 409/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 410/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 411/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 412/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 413/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 414/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 415/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 416/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 417/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 418/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 419/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 420/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 421/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 422/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 423/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 424/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 425/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 426/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 427/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 428/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 429/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 430/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 431/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 432/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 433/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 434/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 435/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 436/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 437/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 438/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 439/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 440/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 441/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 442/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 443/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 444/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 445/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 446/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 447/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 448/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 449/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 450/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 451/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 452/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 453/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 454/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 455/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 456/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 457/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 458/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 459/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 460/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 461/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 462/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 463/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 464/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 465/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 466/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 467/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 468/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 469/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 470/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 471/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 472/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 473/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 474/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 475/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 476/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 477/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 478/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 479/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 480/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 481/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 482/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 483/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 484/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 485/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 486/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 487/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 488/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 489/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 490/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 491/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 492/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 493/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 494/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 495/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 496/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 497/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 498/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 499/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 500/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 501/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 502/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 503/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 504/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 505/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 506/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 507/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 508/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 509/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 510/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 511/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 512/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 513/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 514/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 515/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 516/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 517/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 518/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 519/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 520/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 521/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 522/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 523/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 524/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 525/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 526/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 527/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 528/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 529/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 530/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 531/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 532/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 533/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 534/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 535/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 536/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 537/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 538/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 539/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 540/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 541/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 542/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 543/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 544/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 545/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 546/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 547/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 548/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 549/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 550/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 551/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 552/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 553/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 554/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 555/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 556/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 557/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 558/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 559/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 560/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 561/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 562/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 563/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 564/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 565/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 566/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 567/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 568/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 569/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 570/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 571/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 572/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 573/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 574/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 575/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 576/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 577/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 578/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 579/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 580/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 581/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 582/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 583/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 584/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 585/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 586/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 587/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 588/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 589/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 590/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 591/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 592/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 593/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 594/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 595/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 596/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 597/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 598/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 599/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 600/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 601/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 602/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 603/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 604/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 605/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 606/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 607/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 608/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 609/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 610/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 611/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 612/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 613/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 614/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 615/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 616/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 617/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 618/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 619/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 620/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 621/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 622/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 623/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 624/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 625/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 626/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 627/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 628/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 629/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 630/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 631/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 632/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 633/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 634/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 635/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 636/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 637/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 638/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 639/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 640/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 641/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 642/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 643/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 644/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 645/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 646/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 647/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 648/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 649/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 650/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 651/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 652/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 653/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 654/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 655/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 656/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 657/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 658/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 659/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 660/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 661/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 662/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 663/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 664/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 665/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 666/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 667/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 668/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 669/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 670/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 671/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 672/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 673/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 674/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 675/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 676/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 677/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 678/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 679/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 680/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 681/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 682/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 683/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 684/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 685/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 686/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 687/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 688/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 689/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 690/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 691/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 692/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 693/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 694/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 695/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 696/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 697/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 698/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 699/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 700/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 701/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 702/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 703/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 704/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 705/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 706/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 707/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 708/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 709/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 710/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 711/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 712/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 713/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 714/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 715/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 716/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 717/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 718/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 719/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 720/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 721/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 722/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 723/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 724/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 725/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 726/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 727/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 728/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 729/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 730/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 731/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 732/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 733/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 734/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 735/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 736/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 737/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 738/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 739/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 740/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 741/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 742/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 743/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 744/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 745/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 746/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 747/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 748/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 749/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 750/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 751/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 752/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 753/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 754/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 755/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 756/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 757/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 758/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 759/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 760/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 761/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 762/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 763/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 764/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 765/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 766/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 767/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 768/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 769/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 770/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 771/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 772/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 773/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 774/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 775/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 776/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 777/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 778/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 779/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 780/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 781/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 782/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 783/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 784/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 785/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 786/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 787/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 788/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 789/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 790/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 791/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 792/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 793/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 794/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 795/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 796/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 797/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 798/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 799/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 800/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 801/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 802/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 803/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 804/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 805/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 806/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 807/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 808/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 809/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 810/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 811/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 812/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 813/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 814/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 815/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 816/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 817/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 818/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 819/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 820/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 821/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 822/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 823/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 824/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 825/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 826/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 827/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 828/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 829/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 830/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 831/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 832/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 833/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 834/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 835/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 836/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 837/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 838/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 839/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 840/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 841/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 842/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 843/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 844/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 845/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 846/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 847/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 848/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 849/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 850/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 851/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 852/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 853/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 854/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 855/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 856/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 857/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 858/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 859/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 860/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 861/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 862/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 863/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 864/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 865/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 866/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 867/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 868/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 869/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 870/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 871/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 872/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 873/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 874/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 875/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 876/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 877/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 878/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 879/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 880/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 881/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 882/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 883/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 884/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 885/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 886/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 887/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 888/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 889/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 890/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 891/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 892/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 893/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 894/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 895/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 896/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 897/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 898/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 899/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 900/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 901/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 902/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 903/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 904/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 905/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 906/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 907/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 908/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 909/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 910/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 911/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 912/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 913/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 914/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 915/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 916/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 917/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 918/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 919/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 920/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 921/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 922/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 923/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 924/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 925/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 926/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 927/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 928/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 929/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 930/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 931/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 932/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 933/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 934/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 935/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 936/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 937/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 938/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 939/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 940/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 941/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 942/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 943/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 944/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 945/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 946/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 947/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 948/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 949/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 950/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 951/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 952/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 953/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 954/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 955/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 956/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 957/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 958/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 959/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 960/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 961/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 962/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 963/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 964/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 965/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 966/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 967/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 968/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 969/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 970/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 971/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 972/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 973/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 974/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 975/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 976/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 977/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 978/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 979/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 980/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 981/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 982/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 983/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 984/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 985/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 986/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 987/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 988/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 989/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 990/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 991/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 992/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 993/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 994/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 995/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 996/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 997/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 998/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 999/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "Epoch 1000/1000 - Training Loss: [104895.32425661 104896.70945951 104936.04343574]\n",
            "b,w 63.50,[22.31288333 21.98308932 11.11249919] \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(X, w, b):\n",
        "    y_pred = X.dot(w) + b\n",
        "    return y_pred\n",
        "\n",
        "y_pred = predict(X_test, w_final,b_final)\n",
        "mse = np.mean((y_pred - y_test) ** 2)\n",
        "mse"
      ],
      "metadata": {
        "id": "_R2XsrAgDurd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e80d322b-1fac-4628-8b3a-ba9c634b86ee"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "191463.2449897664"
            ]
          },
          "metadata": {},
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for pred, actual in zip(y_pred[:10], y_test[:10]):\n",
        "    print(f\"{pred}\\t\\t{actual}\")"
      ],
      "metadata": {
        "id": "bW6174SNED4f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9fe974a4-a89d-4153-e1ec-eb3a1dbc944c"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "84.36964401185982\t\t1400.0\n",
            "73.02257652798872\t\t30.0\n",
            "85.03307679957668\t\t23.0\n",
            "88.93212007200964\t\t16.0\n",
            "78.31444417934706\t\t50.0\n",
            "83.62328212567834\t\t16.0\n",
            "73.60308021724097\t\t18.0\n",
            "73.52015111877637\t\t50.0\n",
            "82.79399114103227\t\t18.0\n",
            "90.09312745051415\t\t20.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LinearRegression\n",
        "model = LinearRegression()\n",
        "model.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "GTGZct0vEI3P",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "outputId": "1559c415-7969-4d61-97b8-3fb573c9e557"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LinearRegression()"
            ],
            "text/html": [
              "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LinearRegression()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LinearRegression</label><div class=\"sk-toggleable__content\"><pre>LinearRegression()</pre></div></div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "y_pred2 = model.predict(X_test)\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "mse"
      ],
      "metadata": {
        "id": "powX_teoEM0N",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "390929eb-fd65-4af6-ac39-b41a9cfaa6ff"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "191463.2449897664"
            ]
          },
          "metadata": {},
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for pred, actual in zip(y_pred2[:10], y_test[:10]):\n",
        "    print(f\"{pred}\\t\\t{actual}\")"
      ],
      "metadata": {
        "id": "4ExX4nfaERcB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e4affe9d-1533-4506-c9b2-eb0a21e901c3"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "142.27422233193147\t\t1400.0\n",
            "91.09597479925087\t\t30.0\n",
            "157.92144165066\t\t23.0\n",
            "139.98516146266655\t\t16.0\n",
            "109.35703973405722\t\t50.0\n",
            "124.67110059836189\t\t16.0\n",
            "104.78729170313832\t\t18.0\n",
            "102.83138928829726\t\t50.0\n",
            "105.11207644995125\t\t18.0\n",
            "167.3677952704415\t\t20.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# weights = model.coef_\n",
        "# bias = model.intercept_\n",
        "# weights,bias"
      ],
      "metadata": {
        "id": "efF15-zFEUWK"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "yubRXGYrEYQW"
      },
      "execution_count": 60,
      "outputs": []
    }
  ]
}